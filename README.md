# Autoencoder

Imagine dealing with a dataset of 60,000 data points, where labeling each one is prohibitively expensive. What if we could label only a small portion of it? Is that sufficient for training a neural network? This is precisely where the power of autoencoders comes into play. Instead of individually labeling the entire dataset, a strategic subset of 300 images was selected for training, validation, and testing, showcasing the effectiveness of autoencoders in handling datasets with limited labeled information.

Throughout the process, I applied various data transformations, including random flipping and rotation, to augment the labeled dataset. However, the true breakthrough came from the fundamental working principle of the autoencoder. The key question was whether I could represent the unlabeled data effectively in a smaller latent space. This critical aspect propelled me to secure the **2nd place out of 600** participants on the leaderboard